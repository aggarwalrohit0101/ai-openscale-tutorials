{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"}, {"metadata": {}, "cell_type": "markdown", "source": "# Mitigating Indirect Bias with Watson OpenScale\n\nThis notebook demonstrates the advances concepts of fairness detection using Indirect Bias mechanism using IBM Watson OpenScale.\n\nWe make use of the adult income dataset, where the attributes `age`, `sex` and `race` are not used to train the machine learning model that we are building as part of this notebook. But we will configure fairness against these attributes (specifically `age` and `sex`) to check if the model is indirectly behaving in a biased manner with these protected (sensitive) attributes."}, {"metadata": {}, "cell_type": "markdown", "source": "## Provision services and configure credentials\n\nIf you have not already, provision an instance of IBM Watson OpenScale and an instance of IBM Watson Machine Learning using the Cloud catalog.\n\nYour Cloud API key can be generated by going to the Users section of the Cloud console. From that page, click your name, scroll down to the API Keys section, and click Create an IBM Cloud API key. Give your key a name and click Create, then copy the created key and paste it below.\n\nNOTE: You can also get OpenScale API_KEY using IBM CLOUD CLI.\n\nHow to install IBM Cloud (bluemix) console: [Instructions](https://console.bluemix.net/docs/cli/reference/ibmcloud/download_cli.html#install_use)\n\n<b>How to get api key using console:</b>\n\n<li> bx login --sso\n<li> bx iam api-key-create 'my_key'"}, {"metadata": {}, "cell_type": "markdown", "source": "## Credentials for IBM Cloud services\n\n### Retrieve your IBM Cloud API key\n\n1.\tFrom the IBM Cloud toolbar, click your Account name, such as <Your user name>\u2019s Account.\n1.\tFrom the Manage menu, click Access (IAM).\n1.\tIn the navigation bar, click IBM Cloud API keys.\n1.\tClick the Create an IBM Cloud API key button.\n1.\tType a name and description and then click Save.\n1.\tCopy the newly created API key and paste it into your notebook in the following **CLOUD_API_KEY** code box, which is the first code box.\n\n    Note: replace everything between the two sets of double quotation marks (\")."}, {"metadata": {}, "cell_type": "code", "source": "CLOUD_API_KEY = \"\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Retrieve your Watson Machine Learning credentials\n\n1.\tGo to the IBM Cloud dashboard.\n1.\tIn the Resource summary section, click Services.\n1.\tClick Machine Learning.\n1.\tIn the navigation pane, click Service credentials.\n1.\tClick the New credential button.\n1.\tCopy your credentials by clicking the copy icon.\n1.\tReturn to the notebook editor and update the credentials by replacing the sample credentials with your own in the below cell\n\n   **Note**: You need to replace everything including the opening bracket ({) and the closing bracket (}).\n   \n   **IMPORTANT**: If you are reusing a WML instance that is already bound to Watson OpenScale. Please specify that instance credentials in `wml_credentials`"}, {"metadata": {}, "cell_type": "code", "source": "\nwml_credentials = {\n  \n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "DB_CREDENTIALS = None\nKEEP_MY_INTERNAL_POSTGRES = True\nIAM_URL=\"https://iam.ng.bluemix.net/oidc/token\"", "execution_count": null, "outputs": []}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# Package installation"}, {"metadata": {}, "cell_type": "markdown", "source": "The following opensource packages must be installed into this notebook instance so that they are available to use during processing."}, {"metadata": {}, "cell_type": "code", "source": "!rm -rf $PIP_BUILD\n!pip install --upgrade watson-machine-learning-client --no-cache | tail -n 1\n!pip install --upgrade ibm-ai-openscale --no-cache | tail -n 1\n!pip install psycopg2-binary | tail -n 1\n!pip install pyspark==2.3.0", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Restart the kernel to assure the new libraries are being used."}, {"metadata": {}, "cell_type": "markdown", "source": "# Load and explore data"}, {"metadata": {}, "cell_type": "markdown", "source": "## Explore data"}, {"metadata": {}, "cell_type": "code", "source": "model_name = 'Implicit Bias v2'\ndeployment_name = 'Implicit Bias Deploy v2'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nfrom pyspark import SparkContext, SQLContext\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier,GBTClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, IndexToString\nfrom pyspark.sql.types import StructType, DoubleType, StringType, ArrayType", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#import data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_57b5eb6e7b5d4b90a8e83736129c9010 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='***',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='***')\n\nbody = client_57b5eb6e7b5d4b90a8e83736129c9010.get_object(Bucket='resources-donotdelete-pr-lbyypdyr2le8tz',Key='final_8_noincome.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\npd_data = pd.read_csv(body, sep=\",\", header=0)\npd_data.head()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nfrom pyspark import SparkFiles\n\nspark = SparkSession.builder.getOrCreate()\nspark_df=spark.createDataFrame(pd_data)\n#spark_df = spark.read.csv(path=\"../final_8.csv\", sep=\",\", header=True, inferSchema=True)\nspark_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "si_BusinessTravel = StringIndexer(inputCol='BusinessTravel', outputCol='BusinessTravel_IX')\nsi_Department = StringIndexer(inputCol='Department', outputCol='Department_IX')\nsi_Education = StringIndexer(inputCol='Education', outputCol='Education_IX')\nsi_EducationField = StringIndexer(inputCol='EducationField', outputCol='EducationField_IX')\nsi_RelevantEducationLevel = StringIndexer(inputCol='RelevantEducationLevel', outputCol='RelevantEducationLevel_IX')\nsi_JobRole = StringIndexer(inputCol='JobRole', outputCol='JobRole_IX')\nsi_JobLevel = StringIndexer(inputCol='JobLevel', outputCol='JobLevel_IX')\nsi_MaritalStatus = StringIndexer(inputCol='MaritalStatus', outputCol='MaritalStatus_IX')\nsi_OverTime = StringIndexer(inputCol='OverTime', outputCol='OverTime_IX')\nsi_RequestedBenefits = StringIndexer(inputCol='RequestedBenefits', outputCol='RequestedBenefits_IX')\nsi_PreferredSkills = StringIndexer(inputCol='PreferredSkills', outputCol='PreferredSkills_IX')\nsi_JobType = StringIndexer(inputCol='JobType', outputCol='JobType_IX')\nsi_SalaryExpectation = StringIndexer(inputCol='SalaryExpectation', outputCol='SalaryExpectation_IX')\n\nsi_InterviewScore = StringIndexer(inputCol='InterviewScore', outputCol='InterviewScore_IX')\nsi_ResumeScore = StringIndexer(inputCol='ResumeScore', outputCol='ResumeScore_IX')\n\n#si_MonthlyIncome = StringIndexer(inputCol='MonthlyIncome', outputCol='MonthlyIncome_IX')\n\nsi_Gender = StringIndexer(inputCol='Gender', outputCol='Gender_IX')\nsi_Ethnicity = StringIndexer(inputCol='Ethnicity', outputCol='Ethnicity_IX')\n\nsi_Label = StringIndexer(inputCol=\"HIRED\", outputCol=\"label\").fit(spark_df)\nlabel_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=si_Label.labels)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#payload sample\n(train_data_payload, test_data_payload) = spark_df.randomSplit([0.9, 0.1], 24)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "columns_to_drop = ['Gender', 'Ethnicity']\nspark_df_tmp = spark_df.drop(*columns_to_drop)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "(train_data, test_data) = spark_df_tmp.randomSplit([0.9, 0.1], 24)\nprint(\"Number of records for training: \" + str(train_data.count()))\nprint(\"Number of records for evaluation: \" + str(test_data.count()))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "va_features = VectorAssembler(\ninputCols=[\"Age\", \"BusinessTravel_IX\", \"Department_IX\", \"DistanceFromHome\",\n           \"Education_IX\", \"EducationField_IX\", \"RelevantEducationLevel_IX\", \"JobLevel_IX\", \"JobRole_IX\"\n           , \"MaritalStatus_IX\",\"NumCompaniesWorked\", \"OverTime_IX\",\n           \"InterviewScore_IX\", \"ResumeScore_IX\", \"RequestedBenefits_IX\", \"TotalWorkingYears\", \"PreferredSkills_IX\",\n           \"YearsAtCurrentCompany\",\"RelevantExperience\",\"JobType_IX\",\"SalaryExpectation_IX\"], outputCol=\"features\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "classifier=GBTClassifier(featuresCol=\"features\")#classifier = RandomForestClassifier(featuresCol=\"features\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pipeline = Pipeline(stages=[si_BusinessTravel, si_Department,si_Education, si_EducationField,si_RelevantEducationLevel, si_JobRole,si_JobLevel, si_MaritalStatus,\n        si_OverTime,si_InterviewScore,si_ResumeScore,si_RequestedBenefits,si_PreferredSkills,si_JobType,si_SalaryExpectation, si_Label, va_features, classifier, label_converter])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model = pipeline.fit(train_data)\npredictions = model.transform(test_data)\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(\"Accuracy = %g\" % accuracy)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Save and deploy the model"}, {"metadata": {}, "cell_type": "code", "source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient\nimport json\n\nwml_client = WatsonMachineLearningAPIClient(wml_credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_props = {\n    wml_client.repository.ModelMetaNames.NAME: \"{}\".format(model_name),\n    wml_client.repository.ModelMetaNames.EVALUATION_METHOD: \"binary\",\n    wml_client.repository.ModelMetaNames.EVALUATION_METRICS: [\n        {\n           \"name\": \"accuracy\",\n           \"value\": accuracy,\n           \"threshold\": 0.8\n        }\n    ]\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model = pipeline.fit(spark_df_tmp)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_uid = None\nmodel_deployment_ids = wml_client.deployments.get_uids()\nfor deployment_id in model_deployment_ids:\n    deployment = wml_client.deployments.get_details(deployment_id)\n    model_uid = deployment['entity']['deployable_asset']['guid']\n    if deployment['entity']['name'] == deployment_name:\n        print('Deleting existing deployment with id', deployment_id)\n        wml_client.deployments.delete(deployment_id)\n        print('Deleting existing model with id', model_uid)\n        wml_client.repository.delete(model_uid)\n\nprint(\"Storing model ...\")\n\npublished_model_details = wml_client.repository.store_model(model=model, meta_props=model_props, training_data=spark_df_tmp, pipeline=pipeline)\nmodel_uid = wml_client.repository.get_model_uid(published_model_details)\nprint(\"Done\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_deployments = wml_client.deployments.get_details()\ndeployment_uid = None\nfor deployment in wml_deployments['resources']:\n    if deployment_name == deployment['entity']['name']:\n        deployment_uid = deployment['metadata']['guid']\n        break\n\nif deployment_uid is None:\n    print(\"Deploying model...\")\n\n    deployment = wml_client.deployments.create(artifact_uid=model_uid, name=deployment_name, asynchronous=False)\n    deployment_uid = wml_client.deployments.get_uid(deployment)\n    \nprint(\"Model id: {}\".format(model_uid))\nprint(\"Deployment id: {}\".format(deployment_uid))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# paylaod import\n\npayload_data = pd.read_csv(body)\npayload_data.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "protected_attributes=['Ethnicity','Gender']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "cols_to_remove = ['HIRED']\ncols_to_remove.extend(protected_attributes)\ncols_to_remove", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Create the meta data frame capturing the sensitive data"}, {"metadata": {}, "cell_type": "code", "source": "meta_df = payload_data[protected_attributes].copy()\nmeta_fields = meta_df.columns.tolist()\nmeta_values = meta_df[meta_fields].values.tolist()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Construct the scoring payload comprising the meta fields"}, {"metadata": {}, "cell_type": "code", "source": "def get_scoring_payload(no_of_records_to_score = 1):\n    meta_payload = {\n        \"fields\": meta_fields,\n        \"values\": meta_values[:no_of_records_to_score]\n    }\n\n    for col in cols_to_remove:\n        if col in payload_data.columns:\n            del payload_data[col] \n\n    fields = payload_data.columns.tolist()\n    values = payload_data[fields].values.tolist()\n\n    payload_scoring = {\"fields\": fields,\"values\": values}\n\n    payload_scoring = {\n        \"fields\": fields,\n        \"values\": values[:no_of_records_to_score],\n        \"meta\": meta_payload\n    }\n\n    #import json\n    #print(json.dumps(payload_scoring, indent=None))    \n    return payload_scoring", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Method to perform scoring"}, {"metadata": {}, "cell_type": "code", "source": "def payload_logging(no_of_records_to_score = 1):\n    payload_scoring = get_scoring_payload(no_of_records_to_score)\n    scoring_endpoint = None\n\n    for deployment in wml_client.deployments.get_details()['resources']:\n        if deployment_uid in deployment['metadata']['guid']:\n            scoring_endpoint = deployment['entity']['scoring_url']\n\n    print(scoring_endpoint)    \n    scoring_response = wml_client.deployments.score(scoring_endpoint, payload_scoring)\n    \n    print('Single record scoring result:', '\\n fields:', scoring_response['fields'], '\\n values: ', scoring_response['values'][0])\n    #print(json.dumps(scoring_response, indent=None))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Score the model and print the scoring response"}, {"metadata": {}, "cell_type": "code", "source": "payload_logging(no_of_records_to_score = 1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Configure OpenScale \n\nThe notebook will now import the necessary libraries and set up a Python OpenScale client."}, {"metadata": {}, "cell_type": "code", "source": "from ibm_ai_openscale import APIClient\nfrom ibm_ai_openscale.engines import *\nfrom ibm_ai_openscale.utils import *\nfrom ibm_ai_openscale.supporting_classes import PayloadRecord, Feature\nfrom ibm_ai_openscale.supporting_classes.enums import *\n\nimport json\nimport requests\nimport base64\nfrom requests.auth import HTTPBasicAuth\nimport time", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Get Watson OpenScale GUID\n\nEach instance of OpenScale has a unique ID. We can get this value using the Cloud API key specified at the beginning of the notebook.\n1. Please update the `url` in the below WOS_CREDENTIALS payload as per the environment that you are using.\n2. Please update the `DASHBOARD_URL` in the below cell as per the environment that you are using."}, {"metadata": {}, "cell_type": "code", "source": "from ibm_ai_openscale.utils import get_instance_guid\n\n\nWOS_GUID = get_instance_guid(api_key=CLOUD_API_KEY)\nWOS_CREDENTIALS = {\n    \"instance_guid\": WOS_GUID,\n    \"apikey\": CLOUD_API_KEY,\n    \"url\": \"https://api.aiopenscale.test.cloud.ibm.com\"\n}\nDASHBOARD_URL = \"https://aiopenscale.test.cloud.ibm.com\"\n\nif WOS_GUID is None:\n    print('Watson OpenScale GUID NOT FOUND')\nelse:\n    print(WOS_GUID)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "ai_client = APIClient(aios_credentials=WOS_CREDENTIALS)\nai_client.version", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Generate an IAM token\n### The following is a function that will generate an IAM access token used to interact with the Watson OpenScale APIs"}, {"metadata": {}, "cell_type": "code", "source": "def generate_access_token():\n    headers={}\n    headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\"\n    headers[\"Accept\"] = \"application/json\"\n    auth = HTTPBasicAuth(\"bx\", \"bx\")\n    data = {\n        \"grant_type\": \"urn:ibm:params:oauth:grant-type:apikey\",\n        \"apikey\": CLOUD_API_KEY\n    }\n    response = requests.post(IAM_URL, data=data, headers=headers, auth=auth)\n    json_data = response.json()\n    iam_access_token = json_data['access_token']\n    return iam_access_token", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Create schema and datamart\n\n### Set up datamart\nWatson OpenScale uses a database to store payload logs and calculated metrics. If database credentials were not supplied above, the notebook will use the free, internal lite database. If database credentials were supplied, the datamart will be created there unless there is an existing datamart and the KEEP_MY_INTERNAL_POSTGRES variable is set to True. If an OpenScale datamart exists in Db2 or PostgreSQL, the existing datamart will be used and no data will be overwritten.\n\nPrior instances of the Income classifier model will be removed from OpenScale monitoring."}, {"metadata": {}, "cell_type": "code", "source": "try:\n    data_mart_details = ai_client.data_mart.get_details()\n    if 'internal_database' in data_mart_details and data_mart_details['internal_database']:\n        if KEEP_MY_INTERNAL_POSTGRES:\n            print('Using existing internal datamart.')\n        else:\n            if DB_CREDENTIALS is None:\n                print('No postgres credentials supplied. Using existing internal datamart')\n            else:\n                print('Switching to external datamart')\n                ai_client.data_mart.delete(force=True)\n                ai_client.data_mart.setup(db_credentials=DB_CREDENTIALS)\n    else:\n        print('Using existing external datamart')\nexcept:\n    if DB_CREDENTIALS is None:\n        print('Setting up internal datamart')\n        ai_client.data_mart.setup(internal_db=True)\n    else:\n        print('Setting up external datamart')\n        try:\n            ai_client.data_mart.setup(db_credentials=DB_CREDENTIALS)\n        except:\n            print('Setup failed, trying Db2 setup')\n            ai_client.data_mart.setup(db_credentials=DB_CREDENTIALS, schema=DB_CREDENTIALS['username'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "data_mart_details = ai_client.data_mart.get_details()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "data_mart_details", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Bind WML machine learning instance as Pre-Prod\n\nWatson OpenScale needs to be bound to the Watson Machine Learning instance to capture payload data into and out of the model. If a binding with name \"WML Pre-Prod\" already exists, this code will delete that binding a create a new one."}, {"metadata": {}, "cell_type": "code", "source": "all_bindings = ai_client.data_mart.bindings.get_details()['service_bindings']\nexisting_binding = False\nfor binding in all_bindings:\n    binding_uid = binding['metadata']['guid']\n    if binding['metadata']['guid'] == wml_credentials['instance_id']:\n        existing_binding = True\n        break\n\nif not existing_binding:\n    binding_uid = ai_client.data_mart.bindings.add('WML Prod', WatsonMachineLearningInstance(wml_credentials))\n    \nbindings_details = ai_client.data_mart.bindings.get_details()\nai_client.data_mart.bindings.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(binding_uid)\nai_client.data_mart.bindings.get_details(binding_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "ai_client.data_mart.bindings.list_assets()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Patch binding as production"}, {"metadata": {}, "cell_type": "code", "source": "headers = {}\nheaders[\"Content-Type\"] = \"application/json\"\nheaders[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n\npayload = [\n {\n   \"op\": \"replace\",\n   \"path\": \"/operational_space_id\",\n   \"value\": \"production\"\n }\n]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "SERVICE_PROVIDER_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/service_providers/{1}\".format(WOS_GUID, binding_uid)\nresponse = requests.patch(SERVICE_PROVIDER_URL, json=payload, headers=headers)\njson_data = response.json()\nprint(json_data)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Remove existing prod subscription"}, {"metadata": {}, "cell_type": "markdown", "source": "### This code removes previous subscription that matches the name German Credit Risk Model - Prod as it is expected this subscription is created only via this notebook."}, {"metadata": {}, "cell_type": "code", "source": "subscriptions_uids = ai_client.data_mart.subscriptions.get_uids()\nfor subscription in subscriptions_uids:\n    sub_name = ai_client.data_mart.subscriptions.get_details(subscription)['entity']['asset']['name']\n    if sub_name == model_name:\n        ai_client.data_mart.subscriptions.delete(subscription)\n        print('Deleted existing subscription for', sub_name)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "feature_columns=list(pd_data.drop(['HIRED','Gender','Ethnicity'],axis=1))\nfeature_columns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "categorical_features = pd_data[feature_columns].select_dtypes(include=['object']).columns.tolist()\ncategorical_features", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "subscription = ai_client.data_mart.subscriptions.add(WatsonMachineLearningAsset(\n    model_uid,\n    binding_uid=binding_uid,\n    problem_type=ProblemType.BINARY_CLASSIFICATION,\n    input_data_type=InputDataType.STRUCTURED,\n    label_column='HIRED',\n    prediction_column='predictedLabel',\n    probability_column='probability',\n    feature_columns = feature_columns,\n    categorical_columns = categorical_features\n))\n\nif subscription is None:\n    print('Subscription already exists; get the existing one')\n    subscriptions_uids = ai_client.data_mart.subscriptions.get_uids()\n    for sub in subscriptions_uids:\n        if ai_client.data_mart.subscriptions.get_details(sub)['entity']['asset']['name'] == model_name:\n            subscription = ai_client.data_mart.subscriptions.get(sub)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "ai_client.data_mart.subscriptions.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "subscription.uid", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Patch the training data reference to the challenger subscription"}, {"metadata": {}, "cell_type": "code", "source": "headers = {}\nheaders[\"Content-Type\"] = \"application/json\"\nheaders[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n\n# training_data_reference = {\n#   \"connection\": {\n#     \"database_name\": \"***\",\n#     \"hostname\": \"***\",\n#     \"port\": \"50001\",\n#     \"password\": \"***\",\n#     \"username\": \"***\",\n#     \"ssl\": True\n#   },\n#   \"location\": {\n#     \"schema_name\": \"NMX87075\",\n#     \"table_name\": \"ADULT_INCOME\"\n#   },\n#   \"type\": \"db2\"\n# }\n\ntraining_data_reference = {\n                \"connection\": {\n                    \"api_key\": \"***\",\n                    \"iam_url\": \"https://iam.cloud.ibm.com/oidc/token\",\n                    \"resource_instance_id\": \"***\",\n                    \"url\": \"https://s3.us.cloud-object-storage.appdomain.cloud\"\n                },\n                \"location\": {\n                    \"bucket\": \"***\",\n                    \"file_format\": \"csv\",\n                    \"file_name\": \"final_8_noincome.csv\",\n                    \"firstlineheader\": True,\n                    \"infer_schema\": \"1\"\n                },\n                \"type\": \"cos\"\n            }\n\npayload = [\n {\n   \"op\": \"replace\",\n   \"path\": \"/asset_properties/training_data_reference\",\n   \"value\": training_data_reference\n }\n]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "SUBSCRIPTION_URL = WOS_CREDENTIALS[\"url\"] + \"/v1/data_marts/{0}/service_bindings/{1}/subscriptions/{2}\".format(WOS_GUID, binding_uid, subscription.uid)\n\nresponse = requests.patch(SUBSCRIPTION_URL, json=payload, headers=headers)\njson_data = response.json()\nprint(json_data)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Score the model so we can configure monitors"}, {"metadata": {}, "cell_type": "markdown", "source": "Now that the WML service has been bound and the subscription has been created, we need to send a request to the model before we configure OpenScale. This allows OpenScale to create a payload log in the datamart with the correct schema, so it can capture data coming into and out of the model. First, the code gets the model deployment's endpoint URL, and then sends a few records for predictions."}, {"metadata": {}, "cell_type": "code", "source": "payload_logging(no_of_records_to_score = 100)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "time.sleep(2)\nsubscription.payload_logging.get_records_count()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Fairness, drift monitoring and explanations\n###  Fairness configuration"}, {"metadata": {}, "cell_type": "markdown", "source": "The code below configures fairness monitoring for our model. It turns on monitoring for two features, sex and age. In each case, we must specify:"}, {"metadata": {}, "cell_type": "markdown", "source": "Which model feature to monitor One or more majority groups, which are values of that feature that we expect to receive a higher percentage of favorable outcomes One or more minority groups, which are values of that feature that we expect to receive a higher percentage of unfavorable outcomes The threshold at which we would like OpenScale to display an alert if the fairness measurement falls below (in this case, 80%) Additionally, we must specify which outcomes from the model are favourable outcomes, and which are unfavourable. We must also provide the number of records OpenScale will use to calculate the fairness score. In this case, OpenScale's fairness monitor will run hourly, but will not calculate a new fairness rating until at least 100 records have been added. Finally, to calculate fairness, OpenScale must perform some calculations on the training data, so we provide the dataframe containing the data."}, {"metadata": {}, "cell_type": "markdown", "source": "### Create Fairness Monitor Instance"}, {"metadata": {}, "cell_type": "code", "source": "headers = {}\nheaders[\"Content-Type\"] = \"application/json\"\nheaders[\"Authorization\"] = \"Bearer {}\".format(generate_access_token())\n\nMONITOR_INSTANCES_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/monitor_instances\".format(WOS_GUID)\nMONITOR_INSTANCES_URL", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fairness_paylaod = {\n    \"data_mart_id\": WOS_GUID,\n    \"monitor_definition_id\": \"fairness\",\n    \"parameters\": {\n        \"features\": [\n            {\n                \"feature\": \"Gender\",\n                \"majority\": [\"Male\"],\n                \"minority\": [\"Female\"]\n            },\n            {\n                \"feature\": \"Ethnicity\",\n                \"majority\": [\"non-minority\"],\n                \"minority\": [\"minority\"]\n            }\n        ],\n        \"favourable_class\": [\"YES\"],\n        \"unfavourable_class\": [\"NO\"],\n        \"min_records\": 100\n    },\n    \"target\": {\n        \"target_type\": \"subscription\",\n        \"target_id\": subscription.uid\n    },\n    \"thresholds\": [\n        {\n            \"metric_id\": \"fairness_value\",\n            \"specific_values\": [\n                \n                {\n                    \"applies_to\": [\n                        {\n                            \"type\": \"tag\",\n                            \"value\": \"Gender\",\n                            \"key\": \"feature\"\n                        }\n                    ],\n                    \"value\": 80\n                },\n                {\n                    \"applies_to\": [\n                        {\n                            \"type\": \"tag\",\n                            \"value\": \"Ethnicity\",\n                            \"key\": \"feature\"\n                        }\n                    ],\n                    \"value\": 80\n                }\n            ],\n            \"type\": \"lower_limit\",\n            \"value\": 80\n        }\n    ]\n}\n#print(json.dumps(fairness_paylaod, indent=2))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "response = requests.post(MONITOR_INSTANCES_URL, json=fairness_paylaod, headers=headers, verify=False)\njson_data = response.json()\nprint()\nprint(json_data)\nprint()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Get Fairness Monitor Instance"}, {"metadata": {}, "cell_type": "code", "source": "MONITOR_INSTANCES_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/monitor_instances?target.target_id={1}&target.target_type=subscription\".format(WOS_GUID, subscription.uid)\nprint(MONITOR_INSTANCES_URL)\n\nresponse = requests.get(MONITOR_INSTANCES_URL, headers=headers)\nmonitor_instances = response.json()[\"monitor_instances\"]\n\nfairness_monitor_instance_id = None\n\nif monitor_instances is not None:\n    for monitor_instance in monitor_instances:\n        if \"entity\" in monitor_instance and \"monitor_definition_id\" in monitor_instance[\"entity\"]:\n            monitor_name = monitor_instance[\"entity\"][\"monitor_definition_id\"]\n            if \"metadata\" in monitor_instance and \"id\" in monitor_instance[\"metadata\"]:\n                id = monitor_instance[\"metadata\"][\"id\"]\n                if monitor_name == \"fairness\":\n                    fairness_monitor_instance_id = id\n                    \nprint(\"Fairness monitor instance id - {0}\".format(fairness_monitor_instance_id))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Get existing fairness monitoring run id\n\nWhen we configure fairness monitor, the underlying OpenScale fairness service will submit the first evaluation. For this evaluation in the below cell, we are trying to get the monitoring run it."}, {"metadata": {}, "cell_type": "code", "source": "# sleep for few seconds for the fairneess monitoring check to be submitted.\ntime.sleep(2)\n\ndef get_monitoring_run_id(monitor_instance_id):\n    monitoring_run_id = None\n    MONITOR_INSTANCE_RUNS_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/monitor_instances/{1}/runs\".format(WOS_GUID, monitor_instance_id)\n    response = requests.get(MONITOR_INSTANCE_RUNS_URL, headers=headers, verify=False)\n    monitoring_runs = response.json()\n    \n    # there can be only one run\n    if monitoring_runs is not None and \"runs\" in monitoring_runs:\n        for run in monitoring_runs[\"runs\"]:\n            if \"metadata\" in run and \"id\" in run[\"metadata\"]:\n                monitoring_run_id = run[\"metadata\"][\"id\"]\n                break\n    return monitoring_run_id\n        \nfairness_monitoring_run_id = get_monitoring_run_id(fairness_monitor_instance_id)\nprint(\"Fairness monitoring run id - {0}\".format(fairness_monitoring_run_id))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Function to get the monitoring run details"}, {"metadata": {}, "cell_type": "code", "source": "def get_monitoring_run_details(monitor_instance_id, monitoring_run_id):\n    \n    MONITORING_RUNS_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/monitor_instances/{1}/runs/{2}\".format(WOS_GUID, monitor_instance_id, monitoring_run_id)\n    response = requests.get(MONITORING_RUNS_URL, headers=headers, verify=False)\n    return response.json()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from datetime import datetime\n\ndef check_fairness_run_status(fairness_monitor_instance_id, fairness_monitoring_run_id):\n    fairness_run_status = None\n    while fairness_run_status != 'finished':\n        monitoring_run_details = get_monitoring_run_details(fairness_monitor_instance_id, fairness_monitoring_run_id)\n        fairness_run_status = monitoring_run_details[\"entity\"][\"status\"][\"state\"]\n        if fairness_run_status == \"error\":\n            print(monitoring_run_details)\n            break\n        if fairness_run_status != 'finished':\n            print(datetime.utcnow().strftime('%H:%M:%S'), fairness_run_status)\n            time.sleep(10)\n    print(fairness_run_status)\n\ncheck_fairness_run_status(fairness_monitor_instance_id, fairness_monitoring_run_id)\n\n# fairness_run_details = get_monitoring_run_details(fairness_monitor_instance_id, fairness_monitoring_run_id)\n# print(json.dumps(fairness_run_details, indent = 2))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "FAIRNESS_DASHBOARD_URL = DASHBOARD_URL + \"/aiopenscale/insights/{0}/fairness/Gender?features=fairnessv2,indirect_bias,v2transaction\".format(deployment_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from IPython.display import Markdown as md\nmd(\"#### Link to IBM Watson OpenScale Fairness Dashboard: {}\".format(FAIRNESS_DASHBOARD_URL))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Run on-demand Fairness\nIf you would like to peform an on-demand fairness check, then we need to score a fresh set of data with meta-fields, so that they would be used for indirect bias checking. So the below two cells will score and make sure these records are reached to payload logging table."}, {"metadata": {}, "cell_type": "code", "source": "payload_logging(no_of_records_to_score = 200)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "time.sleep(2)\nsubscription.payload_logging.get_records_count()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Submit the fairness check"}, {"metadata": {}, "cell_type": "code", "source": "if fairness_monitor_instance_id is not None:\n    MONITOR_RUN_URL = WOS_CREDENTIALS[\"url\"] + \"/openscale/{0}/v2/monitor_instances/{1}/runs\".format(WOS_GUID, fairness_monitor_instance_id)\n    payload = {\n        \"triggered_by\": \"user\"\n    }\n    print(\"Triggering Fairness computation with {}\".format(MONITOR_RUN_URL))\n    response = requests.post(MONITOR_RUN_URL, json=payload, headers=headers, verify=False)\n    json_data = response.json()\n    print()\n    print(json_data)\n    print()\n    if \"metadata\" in json_data and \"id\" in json_data[\"metadata\"]:\n        fairness_monitoring_run_id = json_data[\"metadata\"][\"id\"]\n    print(\"Done triggering Fairness check\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Check for its status"}, {"metadata": {}, "cell_type": "code", "source": "check_fairness_run_status(fairness_monitor_instance_id, fairness_monitoring_run_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from IPython.display import Markdown as md\nmd(\"#### To view the latest evaluation of the fairness check, please visit IBM Watson OpenScale Fairness Dashboard: {}\".format(FAIRNESS_DASHBOARD_URL))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Additional data to help debugging"}, {"metadata": {}, "cell_type": "code", "source": "print(\"Model id: {}\".format(model_uid))\nprint(\"Deployment id: {}\".format(deployment_uid))\nprint(\"OpenScale Datamart id: {}\".format(WOS_GUID))\nprint(\"OpenScale Subscription id: {}\".format(subscription.uid))\nprint(\"OpenScale Fairness Monitor Instance id: {}\".format(fairness_monitor_instance_id))\nprint(\"OpenScale Fairness Monitoring Run id: {}\".format(fairness_monitoring_run_id))\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}